{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python-3.6.8/lib/python3.6/site-packages/selenium/webdriver/phantomjs/webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n",
      "  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '\n"
     ]
    }
   ],
   "source": [
    "# 網頁共有幾頁\n",
    "def get_page():\n",
    "    page_source = bs(browser.page_source,'html.parser')\n",
    "    page = page_source.find_all('center')[0]\n",
    "    # 查找last是否存在\n",
    "    if len(page.select('.p_last')):\n",
    "        page = page.select('.p_last')[0]\n",
    "        page = int(page['href'][-1])\n",
    "    else :\n",
    "        page = 1\n",
    "        \n",
    "    return page\n",
    "\n",
    "# get 全部網址\n",
    "def get_url():\n",
    "    page = get_page()\n",
    "\n",
    "    for p in range(page):\n",
    "        soup = bs(browser.page_source,'html.parser')\n",
    "        \n",
    "        for ele in range(len(soup.select('#newslistul li a.tit'))):\n",
    "            title.append(soup.select('#newslistul li a.tit')[ele].text.strip('\\n'))\n",
    "            url.append(soup.select('#newslistul li a.tit')[ele]['href'])\n",
    "            time.sleep(3)\n",
    "\n",
    "        # 若是爬完本頁title及網址，則按下一頁\n",
    "        if p < page-1 :\n",
    "            browser.find_element_by_class_name('p_next').click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "    return title, url\n",
    "\n",
    "# get 全部新聞內文\n",
    "## 利用網址進去網頁爬內文\n",
    "\n",
    "def get_news(url):\n",
    "    # 使用假 header\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}\n",
    "\n",
    "    for u in range(len(url)):\n",
    "        urls = 'https://news.ltn.com.tw/' + str(url[u])\n",
    "        web_data = requests.get(urls, headers=headers)\n",
    "        web_data.encoding = 'utf-8'\n",
    "        soup = bs(web_data.text,'html.parser')\n",
    "\n",
    "        # 標題\n",
    "        data_title = soup.title.text\n",
    "        all_title.append(data_title)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 日期\n",
    "        if len(soup.select('.text span')):\n",
    "            data_date = soup.select('.text span')[0].text\n",
    "            data_date = data_date.strip()\n",
    "            data_date = pd.to_datetime(data_date)\n",
    "            data_date = data_date.date()\n",
    "            date.append(data_date)\n",
    "\n",
    "        elif len(soup.select('.c_time')):\n",
    "            data_date = soup.select('.c_time')[0].text\n",
    "            data_date = data_date.strip()\n",
    "            data_date = pd.to_datetime(data_date)\n",
    "            data_date = data_date.date()\n",
    "            date.append(data_date)\n",
    "\n",
    "        elif len(soup.select('.date')):\n",
    "            data_date = soup.select('.date')[0].text\n",
    "            data_date = data_date.strip()\n",
    "            data_date = pd.to_datetime(data_date)\n",
    "            data_date = data_date.date()\n",
    "            date.append(data_date)\n",
    "\n",
    "        elif len(soup.select('.writer_date')):\n",
    "            data_date = soup.select('.writer_date')[0].text\n",
    "            data_date = data_date.strip()\n",
    "            data_date = pd.to_datetime(data_date)\n",
    "            data_date = data_date.date()\n",
    "            date.append(data_date)\n",
    "\n",
    "        elif len(soup.select('.viewtime')):\n",
    "            data_date = soup.select('.viewtime')[0].text\n",
    "            data_date = data_date.strip()\n",
    "            data_date = pd.to_datetime(data_date)\n",
    "            data_date = data_date.date()\n",
    "            date.append(data_date)\n",
    "    \n",
    "        else :\n",
    "            print('date:',u)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 內文\n",
    "        one_sentence = []\n",
    "\n",
    "        if len(soup.select('.text p')):\n",
    "            for i in range(len(soup.select('.text p'))):\n",
    "                # 搜集\"還想看更多新聞嗎？\"前的所有文字\n",
    "                if soup.select('.text p')[i].text != soup.select('.appE1121')[0].text :\n",
    "                    one_sentence.append(soup.select('.text p')[i].text)\n",
    "                else :\n",
    "                    break\n",
    "\n",
    "        elif len(soup.select('.cont p')):\n",
    "            for i in range(len(soup.select('.cont p'))):\n",
    "                # 搜集\"還想看更多新聞嗎？\"前的所有文字\n",
    "                if soup.select('.cont p')[i].text != soup.select('.appE1121')[0].text :\n",
    "                    one_sentence.append(soup.select('.cont p')[i].text)\n",
    "                else :\n",
    "                    break\n",
    "                    \n",
    "        elif len(soup.select('.news_content p')):\n",
    "            for i in range(len(soup.select('.news_content p'))):\n",
    "                # 搜集\"還想看更多新聞嗎？\"前的所有文字\n",
    "                if soup.select('.news_content p')[i].text != soup.select('.appE1121')[0].text :\n",
    "                    one_sentence.append(soup.select('.news_content p')[i].text)\n",
    "                else :\n",
    "                    break\n",
    "                    \n",
    "        elif len(soup.select('p')):\n",
    "            s_sele = soup.select('p')[5:16]\n",
    "            select_p = []\n",
    "            for s in range(len(s_sele)):\n",
    "                if (s % 2) == 0:\n",
    "                    select_p.append(s_sele[s])\n",
    "\n",
    "            for i in range(len(select_p)):\n",
    "                one_sentence.append(select_p[i].text)\n",
    "            \n",
    "        else :\n",
    "            print('content:',u)\n",
    "\n",
    "        # 把同一篇的句子合併\n",
    "        sentence = ''.join(one_sentence)\n",
    "        content.append(sentence)\n",
    "        time.sleep(3)\n",
    "\n",
    "    return all_title, date, content\n",
    "\n",
    "s_Year = ['2016','2017','2018']\n",
    "s_Month = ['1', '4', '7', '9', '12']\n",
    "e_Month = ['3', '6', '8', '11', '12']\n",
    "e_Day = ['31', '30']\n",
    "\n",
    "title = []\n",
    "url = []\n",
    "\n",
    "all_title = []\n",
    "date = []\n",
    "content = []\n",
    "\n",
    "browser = webdriver.PhantomJS('C:/Users/joyle/Anaconda3/Scripts/phantomjs.exe')\n",
    "# browser = webdriver.PhantomJS()\n",
    "browser.get('https://news.ltn.com.tw/search?keyword=')\n",
    "browser.find_element_by_id('keyword_search').send_keys(\"澳幣\")\n",
    "\n",
    "for i in range(len(s_Year)):\n",
    "    # 選擇起始年份\n",
    "    SYear = Select(browser.find_element_by_id('SYear'))\n",
    "    SYear.select_by_value(s_Year[i])\n",
    "    # 選擇結束年份\n",
    "    EYear = Select(browser.find_element_by_id('EYear'))\n",
    "    EYear.select_by_value(s_Year[i])\n",
    "    for y in range(len(s_Month)):\n",
    "        # 選擇起始月份\n",
    "        SMonth = Select(browser.find_element_by_id('SMonth'))\n",
    "        SMonth.select_by_value(s_Month[y])\n",
    "        # 選擇起始日期\n",
    "        SDay = Select(browser.find_element_by_id('SDay'))\n",
    "        SDay.select_by_value(\"1\")\n",
    "\n",
    "        #選擇結束月份\n",
    "        EMonth = Select(browser.find_element_by_id('EMonth'))\n",
    "        EMonth.select_by_value(e_Month[y])\n",
    "        if ( y % 2 ) == 0 :\n",
    "            #選擇結束日期\n",
    "            EDay = Select(browser.find_element_by_id('EDay'))\n",
    "            EDay.select_by_value(e_Day[0])\n",
    "            day = e_Day[0]\n",
    "        else :\n",
    "            EDay = Select(browser.find_element_by_id('EDay'))\n",
    "            EDay.select_by_value(e_Day[1])\n",
    "            day = e_Day[1]\n",
    "\n",
    "       # print('起始日期:',s_Year[i], s_Month[y],'1','結束日期:',s_Year[i],e_Month[y],day)\n",
    "\n",
    "        browser.find_element_by_xpath('/html/body/div[3]/section/div[3]/div[2]/form/input').click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        title, url = get_url()\n",
    "\n",
    "# browser.save_screenshot('news1.png') #螢幕截圖        \n",
    "browser.quit() # 離開網頁\n",
    "\n",
    "all_title, date, content = get_news(url)\n",
    "\n",
    "date.sort()\n",
    "all_title = pd.DataFrame(all_title)\n",
    "date = pd.DataFrame(date)\n",
    "content = pd.DataFrame(content)\n",
    "\n",
    "all_title.columns = ['title']\n",
    "date.columns = ['date']\n",
    "content.columns = ['content']\n",
    "\n",
    "df = pd.concat([date, all_title, content], axis = 1)\n",
    "df.to_csv('news_2016-2018.csv',index=False, encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
